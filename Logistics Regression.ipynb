{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNUOqCuki4qknRSA6rLk52K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"oEdKC1Ft_Zp4","executionInfo":{"status":"ok","timestamp":1698559431571,"user_tz":300,"elapsed":123,"user":{"displayName":"Saket Mugunda","userId":"11254807757958452093"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import lightgbm as lgb\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC  # Import the Support Vector Classifier\n","\n","# Assuming X_train and y_train are your training data\n","# For SVM, you might want to scale your data\n","# Here's an example using StandardScaler from sklearn\n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"code","source":["data = pd.read_csv('cleandata.csv')\n","data = data.drop(data.columns[0], axis=1)\n","X = data.drop('death', axis=1)\n","y = data['death']\n","\n","cat_columns = ['sex', 'race', 'dnr', 'primary', 'disability', 'income', 'extraprimary', 'cancer']"],"metadata":{"id":"-6GZ0Wfb_jUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PG2WCWBFCfpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X[cat_columns] = X[cat_columns].astype(\"category\")"],"metadata":{"id":"t2ARl8_B_oS9","executionInfo":{"status":"aborted","timestamp":1698559431677,"user_tz":300,"elapsed":7,"user":{"displayName":"Saket Mugunda","userId":"11254807757958452093"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"oXU8I-sRAS1P","executionInfo":{"status":"aborted","timestamp":1698559431677,"user_tz":300,"elapsed":7,"user":{"displayName":"Saket Mugunda","userId":"11254807757958452093"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.dtypes"],"metadata":{"id":"SRmLIZQdATVg","executionInfo":{"status":"aborted","timestamp":1698559431678,"user_tz":300,"elapsed":8,"user":{"displayName":"Saket Mugunda","userId":"11254807757958452093"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Use the Logistic Regression model\n","model = LogisticRegression()\n","\n","\n","# Fit the model using the training data\n","model.fit(X_train, y_train)"],"metadata":{"id":"mYkGE7izAVlZ","executionInfo":{"status":"aborted","timestamp":1698559431678,"user_tz":300,"elapsed":7,"user":{"displayName":"Saket Mugunda","userId":"11254807757958452093"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(X_test)"],"metadata":{"id":"_vOc-Wa5_8Sj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred"],"metadata":{"id":"RWa8uKZ6ABRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy = accuracy_score(y_test, y_pred)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","norm_conf_matrix = confusion_matrix(y_test,y_pred,normalize='true')"],"metadata":{"id":"BN5uQogJADF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy"],"metadata":{"id":"WmrMI94rAMZS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conf_matrix"],"metadata":{"id":"hzIe7oLHAO8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set(font_scale=1.2)\n","\n","# Create a heatmap of the confusion matrix\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True,\n","            xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n","            yticklabels=[\"Actual 0\", \"Actual 1\"])\n","\n","# Add labels and a title\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix Heatmap')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"KU-GgwqiARtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set(font_scale=1.2)\n","\n","# Create a heatmap of the confusion matrix\n","sns.heatmap(norm_conf_matrix, annot=True, cmap=\"Blues\", cbar=False, square=True,\n","            xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n","            yticklabels=[\"Actual 0\", \"Actual 1\"])\n","\n","# Add labels and a title\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix Heatmap')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"Lzg9B0ZrAduL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Assuming 'model' is a trained Logistic Regression model\n","feature_importance = model.coef_[0]  # Extracting the coefficients of the model\n","\n","# Matching coefficients with feature names\n","feature_names = X_train.columns  # Assuming X_train is your training data after one-hot encoding\n","\n","# Creating a dataframe to store feature names and their importance (coefficients)\n","feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n","feature_importance_df['AbsImportance'] = feature_importance_df['Importance'].abs()\n","\n","# Sorting features based on absolute coefficient values\n","feature_importance_df = feature_importance_df.sort_values(by='AbsImportance', ascending=False)\n","\n","# Plotting feature importance\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))  # Plotting top 10 features\n","plt.title('Feature Importance (Logistic Regression)')\n","plt.xlabel('Coefficient')\n","plt.ylabel('Feature')\n","plt.show()\n","#This code snippet extracts the coefficients from the trained logistic regression model and visualizes the top important features based on the absolute magnitude of the coefficients using a bar plot.\n","\n","#Please note that interpreting feature importance in logistic regression is based on the coefficients' magnitude. Features with larger coefficients (either positive or negative) have a more significant impact on the model's predictions. Adjust the number of features to be displayed as needed by changing the argument in head() function.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"xjIM2g0YAzhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explainer = shap.TreeExplainer(model)\n","shap_values = explainer.shap_values(X)"],"metadata":{"id":"PNLpsvCNA_qQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'model' is a trained Logistic Regression model\n","shap_explainer = shap.Explainer(model, X_train)\n","shap_values = shap_explainer(X_test)\n","\n","# Summarizing SHAP values\n","shap.summary_plot(shap_values, X_test, plot_type='bar')"],"metadata":{"id":"mD-iQQ87BA6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n","roc_auc = auc(fpr, tpr)"],"metadata":{"id":"_kmotG3PBKp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","plt.step(recall, precision, color='b', where='post', lw=2)\n","plt.fill_between(recall, precision, alpha=0.2, color='b')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.ylim([0.0, 1.05])\n","plt.xlim([0.0, 1.0])\n","plt.title('Precision-Recall Curve (AP = {:.2f})'.format(average_precision))\n","plt.show()"],"metadata":{"id":"-bIXLD1wBLen"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n","average_precision = average_precision_score(y_test, y_pred)"],"metadata":{"id":"bWArWVHKBTFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","plt.step(recall, precision, color='b', where='post', lw=2)\n","plt.fill_between(recall, precision, alpha=0.2, color='b')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.ylim([0.0, 1.05])\n","plt.xlim([0.0, 1.0])\n","plt.title('Precision-Recall Curve (AP = {:.2f})'.format(average_precision))\n","plt.show()"],"metadata":{"id":"j40jG2JeCDTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# n_splits = 5\n","# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"],"metadata":{"id":"txwJlTwsCHl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n","#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n","#     y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n","\n","#     train_data = lgb.Dataset(X_train, label=y_train)\n","#     valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n","\n","#     params['verbose'] = 100  # Adjust verbosity here\n","\n","#     model = lgb.train(params, train_data, valid_sets=[train_data, valid_data])\n","\n","#     y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n","#     y_pred_binary = np.round(y_pred)  # Convert to binary predictions\n","\n","#     accuracy = accuracy_score(y_valid, y_pred_binary)\n","#     print(f\"Fold {fold + 1}, Accuracy: {accuracy}\")\n","\n","#     # You can save or use the models for further analysis\n","#     # model.save_model(f\"model_fold_{fold + 1}.txt\")"],"metadata":{"id":"Xa_EJfddCMkM"},"execution_count":null,"outputs":[]}]}